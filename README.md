# 2023 experiment: Human-Robot Interaction(HRI) Study

## 1. Background:
The overarching goal of this project is to delve deeper into human social cognition through human-robot interaction (HRI) experiments. We aim to understand the distinct features of a social interaction that make humans recognize they are interacting with an intelligent agent with its own intentions, as opposed to a pre-programmed machine. Despite the advancements in AI and robotics, there remains a clear distinction in human perception when interacting with another human and an artificial agent. This project seeks to uncover the reasons behind this distinction, which could pave the way for more "life-like" interactive AI in the future.

## 2. Research Objectives:
1. Understand the natural interaction dynamics of humans in a social environment.
2. Adopt a data-driven approach to develop the Social AI, focusing on elements of the interaction that influence social fluidity.

## 3. Experimental Setup:
Participants are brought into our lab individually to play a cooperative video game with a virtual avatar. This avatar could either be controlled by an artificial agent (using the Social AI) or a trained human confederate. The game of choice is a customized version of "Don't Starve Together", a social survival game. The Social AI is equipped with a Speech Dialogue system that can react to in-game events through autonomously-generated speech using a text-to-speech (TTS) module in both English and Korean.

## 4. Hypotheses Under Investigation:
1. **Turn-Taking Prediction (H1)**: This hypothesis tests the effect of the Social AI avatar's ability to predict when it's its turn to speak. The prediction is based on a multi-modal transformer model of turn-taking interaction cues. This model has been discussed in detail in our scholarly paper titled "Real-Time Multimodal Turn-taking Prediction to Enhance Cooperative Dialogue during Human-Agent Interaction".
   
2. **Repeated Language Code-Switching (H2)**: Here, we test the effect of repeated language code-switching during a single experiment. The avatar switches languages approximately every 3 minutes during the 30-minute experiment.

## 5. Key Findings from the Scholarly Paper:
- Developed a real-time multimodal turn-taking prediction model to enhance cooperative dialogue during human-agent interaction.
- The model was trained on a dataset from human-human interactions and evaluated in a human-agent interaction scenario.
- The model showcased promising results in predicting turn-taking events in real-time, outperforming other baseline models.

## 6. Further Reading:
For a comprehensive understanding of our turn-taking prediction model and its implications, please refer to our scholarly paper: [Real-Time Multimodal Turn-taking Prediction to Enhance Cooperative Dialogue during Human-Agent Interaction](https://drive.google.com/file/d/1Kh-XhDySf9iaQDU4lKtb9ckFcuWw3C1m/view?usp=drive_link).

## 7. Acknowledgements
This work was supported by a grant from the National Research Foundation of Korea (NRF) (Grant number:2021R1G1A1003801).
